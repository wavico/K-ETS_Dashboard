import streamlit as st
import sys
import os
import pdfplumber
from openai import OpenAI
from datetime import datetime
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone as PineconeVectorStore
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from io import BytesIO
from docx import Document
from docx.shared import Pt
from pinecone import Pinecone


# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ utils ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI_ë¦¬í¬íŠ¸ ìƒì„±ê¸°",
    page_icon="ğŸ“‹",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSS
st.markdown("""
<style>
    .info-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 25px;
        border-radius: 15px;
        color: white;
        margin: 20px 0;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    .data-source-card {
        background: white;
        padding: 20px;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        margin: 15px 0;
        border-left: 5px solid #1f77b4;
    }
    .update-card {
        background: linear-gradient(135deg, #74b9ff 0%, #0984e3 100%);
        padding: 15px;
        border-radius: 10px;
        color: white;
        margin: 10px 0;
    }
    .system-info {
        background: linear-gradient(135deg, #00b894 0%, #00a085 100%);
        padding: 20px;
        border-radius: 15px;
        color: white;
        margin: 15px 0;
    }
    .guide-section {
        background: #f8f9fa;
        padding: 20px;
        border-radius: 10px;
        margin: 15px 0;
        border: 1px solid #e9ecef;
    }
</style>
""", unsafe_allow_html=True)

# íƒ€ì´í‹€
st.markdown('<h1 class="main-header">ğŸ“„ AI ê¸°ë°˜ ë³´ê³ ì„œ ìƒì„±ê¸°</h1>', unsafe_allow_html=True)

# OpenAI API Key ë¡œë”©
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_env=os.getenv("PINECONE_ENV")
pc = Pinecone(api_key=pinecone_api_key)
index_name = "carbone-index"

# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
@st.cache_data
def extract_text_from_pdf(uploaded_file):
    with pdfplumber.open(uploaded_file) as pdf:
        text = ""
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# ëª©ì°¨ ì¶”ì¶œ í•¨ìˆ˜
def extract_table_of_contents(text):
    prompt = f"""
ë‹¤ìŒ ë¬¸ì„œì—ì„œ **ëª©ì°¨(ì°¨ë¡€)**ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë§Œ ì •í™•íˆ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.
- ìˆ«ìë‚˜ ë¡œë§ˆì, ì œëª© íŒ¨í„´ì„ ì´ìš©í•´ ëª©ì°¨ í•­ëª©ë§Œ ë½‘ì•„ì£¼ì„¸ìš”.
- ë³¸ë¬¸ ë‚´ìš©ì€ í¬í•¨í•˜ì§€ ë§ê³ , ëª©ì°¨ êµ¬ì¡°ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{text[:4000]}
"""
    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ êµ¬ì¡°ì—ì„œ ëª©ì°¨ë§Œ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )
    return response.choices[0].message.content.strip()

# ë¬¸ì„œ ì „ì²´ êµ¬ì¡° ìš”ì•½
def summarize_template_structure(text):
    prompt = f"""
ë‹¤ìŒ ë¬¸ì„œì˜ í˜•ì‹(ë³´ê³ ì„œ êµ¬ì¡°, ì œëª© ìŠ¤íƒ€ì¼, êµ¬ì„± íë¦„ ë“±)ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ ì£¼ì„¸ìš”.
- ë¬¸ì„œê°€ ì–´ë–¤ í˜•ì‹ìœ¼ë¡œ ì‘ì„±ë˜ì–´ ìˆëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- ëª©ì°¨, ë³¸ë¬¸ êµ¬ì„±, ì–¸ì–´ í†¤ ë“±ì„ í¬í•¨í•´ í˜•ì‹ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{text[:4000]}
"""
    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ í˜•ì‹ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•˜ëŠ” AIì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5
    )
    return response.choices[0].message.content.strip()

#Pinecone ê¸°ë°˜ RAGìš© DB ìƒì„±
@st.cache_resource
def create_vector_store(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    docs = text_splitter.create_documents([text])
    embeddings = OpenAIEmbeddings()

    # âœ… ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ â†’ ì—†ìœ¼ë©´ ìƒì„±
    # âœ… ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if index_name not in pc.list_indexes().names():
        from pinecone import ServerlessSpec
        pc.create_index(
            name=index_name,
            dimension=1536,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )

    vector_store = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name)
    return vector_store


#Word ë³´ê³ ì„œ ìƒì„± í•¨ìˆ˜
def generate_docx_report(text, topic):
    doc = Document()
    doc.add_heading(topic, level=1)

    for paragraph in text.split("\n"):
        if paragraph.strip():
            p = doc.add_paragraph(paragraph.strip())
            p.style.font.size=Pt(11)
            p.paragraph_format.line_spacing = 1.5

    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

# ë³´ê³ ì„œ ìƒì„±
def generate_report_with_rag(topic, vector_store):
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})
    llm = ChatOpenAI(temperature=0.7, model_name="gpt-4-turbo")
    qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)

    prompt = f"""
'{topic}'ì— ëŒ€í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”. 
- ì°¸ê³  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±ê³¼ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”.
- ëª©ì°¨ êµ¬ì„±, ë§íˆ¬, ë¶„ì„ ë°©í–¥ì„ ìœ ì§€í•˜ê³ , ìƒˆë¡œìš´ ì£¼ì œì— ë§ê²Œ ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”.
- ì°¸ê³  ë¬¸ì„œëŠ” ì°¸ê³ ë§Œ í• ë¿ì´ì§€, ê±°ê¸°ì˜ ë‚´ìš©ì„ ê°€ì§€ê³ ì™€ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.
"""
    return qa.run(prompt)

# # ì£¼ì œ ê¸°ë°˜ ë³´ê³ ì„œ ìƒì„±
# def generate_report_based_on_template(topic, template_text):
#     prompt = f"""
# ì•„ë˜ ë¬¸ì„œ í˜•ì‹ì„ ì°¸ê³ í•˜ì—¬, ìƒˆë¡œìš´ ì£¼ì œ '{topic}'ì— ëŒ€í•´ ë™ì¼í•œ í˜•ì‹ì˜ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.
#
# - ë¬¸ì„œ í˜•ì‹(ëª©ì°¨, êµ¬ì„±, ë§íˆ¬ ë“±)ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜,
# - ë‚´ìš©ì€ '{topic}'ì„ ê¸°ë°˜ìœ¼ë¡œ ì™„ì „íˆ ìƒˆë¡­ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
#
# ğŸ“„ ì°¸ê³  ë¬¸ì„œ:
# {template_text[:4000]}
# """
#     response = client.chat.completions.create(
#         model="gpt-4",
#         messages=[
#             {"role": "system", "content": "ë‹¹ì‹ ì€ ë³´ê³ ì„œ í˜•ì‹ì„ í•™ìŠµí•´ ìƒˆë¡œìš´ ì£¼ì œì— ë§ì¶° ì‘ì„±í•˜ëŠ” AIì…ë‹ˆë‹¤."},
#             {"role": "user", "content": prompt}
#         ],
#         temperature=0.7
#     )
#     return response.choices[0].message.content.strip()

uploaded_file = st.file_uploader("ğŸ“ PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

if uploaded_file:
    with st.spinner("ë¬¸ì„œ ë¶„ì„ ì¤‘..."):
        extracted_text = extract_text_from_pdf(uploaded_file)
        toc = extract_table_of_contents(extracted_text)
        structure_summary = summarize_template_structure(extracted_text)
        vector_store = create_vector_store(extracted_text)

    st.subheader("ğŸ§¾ ë¬¸ì„œ ëª©ì°¨ ìë™ ì¶”ì¶œ")
    st.code(toc, language="markdown")

    st.subheader("ğŸ“š ë¬¸ì„œ í˜•ì‹ ìš”ì•½")
    st.markdown(structure_summary)

    topic = st.text_input("ğŸ“ ìƒˆë¡œ ì‘ì„±í•  ë³´ê³ ì„œ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: íƒ„ì†Œì¤‘ë¦½ ì¶”ì§„ ì „ëµ")

    if topic:
        with st.spinner("ğŸ“„ ìƒˆë¡œìš´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            generated_report = generate_report_with_rag(topic, vector_store)
            docx_file = generate_docx_report(generated_report, topic)

        st.success("âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
        st.download_button(
            label="ğŸ“¥ Word í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (.docx)",
            data=docx_file,
            file_name=f"{topic}_ë³´ê³ ì„œ.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )

        st.text_area("ğŸ“„ ìƒì„±ëœ ë³´ê³ ì„œ ë¯¸ë¦¬ë³´ê¸°", generated_report, height=500)


# topic = st.text_input("ğŸ“ ìƒˆë¡œ ì‘ì„±í•  ë³´ê³ ì„œ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: íƒ„ì†Œì¤‘ë¦½ ì¶”ì§„ ì „ëµ")
#
# # Step 2: ì£¼ì œ ì…ë ¥ í›„ ë³´ê³ ì„œ ìƒì„±
# if uploaded_file and topic:
#     with st.spinner("ğŸ“„ ìƒˆë¡œìš´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
#         generated_report = generate_report_based_on_template(topic, extracted_text)
#     st.success("âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
#
#     st.download_button(
#         "ğŸ“¥ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ",
#         generated_report,
#         file_name=f"{topic}_ë³´ê³ ì„œ.txt",
#         mime="text/plain"
#     )
#
#     st.text_area("ğŸ“„ ìƒì„±ëœ ë³´ê³ ì„œ ë¯¸ë¦¬ë³´ê¸°", generated_report, height=500)