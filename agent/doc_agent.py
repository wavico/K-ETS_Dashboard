"""
ë¬¸ì„œ íŒŒì‹± ë° ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” 'docs' í´ë”ì— ìˆëŠ” PDF ë¬¸ì„œë“¤ì„ ì²˜ë¦¬í•˜ì—¬ LangChainê³¼ Pineconeì„ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œì˜ ê¸°ë°˜ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

ì‹¤í–‰ ìˆœì„œ:
1. 'docs' í´ë”ì—ì„œ PDF íŒŒì¼ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.
2. Upstage Document AI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê° PDFë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ íŒŒì‹±í•˜ê³  HTMLë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
   - í° PDFëŠ” ì‘ì€ í˜ì´ì§€ ë¬¶ìŒìœ¼ë¡œ ìë™ ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
3. íŒŒì‹±ëœ HTML ì½˜í…ì¸ ë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„(ì²­í¬)ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
4. Upstage ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
5. ë³€í™˜ëœ ë²¡í„°ë¥¼ Pinecone ë²¡í„° ì €ì¥ì†Œì˜ 'carbon-rag' ì¸ë±ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
   - ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ ê¸°ì¡´ ì¸ë±ìŠ¤ëŠ” ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•˜ì—¬ í•­ìƒ ìµœì‹  ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import fitz  # PyMuPDF
import requests
import json
from glob import glob
from dotenv import load_dotenv
from pathlib import Path
import shutil

# LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub

# --- 1. í™˜ê²½ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ ---

def setup_environment():
    """í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ê³  API í‚¤ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."""
    load_dotenv()
    upstage_api_key = os.getenv("UPSTAGE_API_KEY")
    pinecone_api_key = os.getenv("PINECONE_API_KEY")
    if not upstage_api_key or not pinecone_api_key:
        raise ValueError("API í‚¤ê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤: UPSTAGE_API_KEY, PINECONE_API_KEY")
    return upstage_api_key, pinecone_api_key

def find_project_root() -> Path:
    """ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    # __file__ì€ í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ìŠ¤í¬ë¦½íŠ¸ì˜ ê²½ë¡œë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    current_path = Path(__file__).resolve()
    # í”„ë¡œì íŠ¸ì˜ ë§ˆì»¤ íŒŒì¼ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤.
    for parent in current_path.parents:
        if (parent / "requirements.txt").exists() and (parent / "chatbot_app.py").exists():
            return parent
    raise FileNotFoundError("í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'requirements.txt'ì™€ 'chatbot_app.py'ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

# --- 2. ë¬¸ì„œ ì „ì²˜ë¦¬ (PDF íŒŒì‹±) ---

def get_pdf_files(docs_folder: Path) -> list[str]:
    """ì§€ì •ëœ í´ë”ì—ì„œ ëª¨ë“  PDF íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not docs_folder.exists():
        raise FileNotFoundError(f"ë¬¸ì„œ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {docs_folder}")
    
    pdf_files = list(docs_folder.glob("*.pdf"))
    if not pdf_files:
        raise ValueError(f"ë¬¸ì„œ í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {docs_folder}")
    
    print(f"ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:")
    for i, pdf_file in enumerate(pdf_files, 1):
        file_size_mb = pdf_file.stat().st_size / (1024 * 1024)
        print(f"  {i}. {pdf_file.name} ({file_size_mb:.1f} MB)")
    
    return [str(f) for f in pdf_files]

def parse_pdf_with_upstage(input_file: str, batch_size: int, upstage_api_key: str) -> str:
    """Upstage APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ PDF íŒŒì¼ì„ HTMLë¡œ íŒŒì‹±í•©ë‹ˆë‹¤."""
    print(f"\n--- PDF ì²˜ë¦¬ ì‹œì‘: {os.path.basename(input_file)} ---")
    
    # ì„ì‹œ ë¶„í•  íŒŒì¼ë“¤ì„ ì €ì¥í•  í´ë” ìƒì„±
    temp_dir = Path(os.path.dirname(input_file)) / "temp_split"
    temp_dir.mkdir(exist_ok=True)

    try:
        input_pdf = fitz.open(input_file)
    except Exception as e:
        print(f"  âŒ PDF íŒŒì¼ì„ ì—¬ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        shutil.rmtree(temp_dir)
        return ""

    # 1ë‹¨ê³„: PDFë¥¼ ì‘ì€ í˜ì´ì§€ ë¬¶ìŒìœ¼ë¡œ ë¶„í• 
    print(f"[1/3] PDFë¥¼ {batch_size} í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¶„í•  ì¤‘...")
    split_files, json_files = [], []
    num_pages = len(input_pdf)
    for start_page in range(0, num_pages, batch_size):
        end_page = min(start_page + batch_size - 1, num_pages - 1)
        output_path = temp_dir / f"{Path(input_file).stem}_{start_page}_{end_page}.pdf"
        with fitz.open() as output_pdf:
            output_pdf.insert_pdf(input_pdf, from_page=start_page, to_page=end_page)
            output_pdf.save(str(output_path))
        split_files.append(str(output_path))
    input_pdf.close()
    print(f"  - {len(split_files)}ê°œì˜ íŒŒì¼ë¡œ ë¶„í•  ì™„ë£Œ.")

    # 2ë‹¨ê³„: ë¶„í• ëœ ê° íŒŒì¼ì— ëŒ€í•´ Upstage API í˜¸ì¶œ
    print("[2/3] Upstage Document API í˜¸ì¶œ ì¤‘...")
    for short_input_file in split_files:
        try:
            with open(short_input_file, "rb") as f:
                response = requests.post(
                    "https://api.upstage.ai/v1/document-digitization",
                    headers={"Authorization": f"Bearer {upstage_api_key}"},
                    data={"ocr": "true"},  # OCR ì˜µì…˜ í™œì„±í™”
                    files={"document": f},
                )
            response.raise_for_status()
            
            json_output_file = Path(short_input_file).with_suffix(".json")
            with open(json_output_file, "w", encoding="utf-8") as f:
                json.dump(response.json(), f, ensure_ascii=False, indent=4)
            json_files.append(str(json_output_file))
            print(f"  - API ì‘ë‹µ ì €ì¥: {json_output_file.name}")
        except requests.exceptions.RequestException as e:
            print(f"  - âŒ API í˜¸ì¶œ ì˜¤ë¥˜ ({Path(short_input_file).name}): {e}")

    # 3ë‹¨ê³„: JSON ê²°ê³¼ì—ì„œ HTML ì½˜í…ì¸  í†µí•©
    print("[3/3] íŒŒì‹±ëœ ì½˜í…ì¸  í†µí•© ì¤‘...")
    full_html_content = ""
    for json_file in json_files:
        with open(json_file, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                html_part = data.get("html", "")
                if html_part:
                    full_html_content += html_part
                    print(f"  - ì½˜í…ì¸  ì¶”ê°€: {Path(json_file).name} ({len(html_part):,} ì)")
            except json.JSONDecodeError:
                print(f"  - âŒ JSON íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {Path(json_file).name}")
    
    # ì„ì‹œ íŒŒì¼ ë° í´ë” ì‚­ì œ
    shutil.rmtree(temp_dir)
    
    print(f"--- PDF ì²˜ë¦¬ ì™„ë£Œ. ì´ {len(full_html_content):,} ìì˜ HTML ì½˜í…ì¸  ì¶”ì¶œ ---")
    return full_html_content

def parse_all_pdfs(pdf_files: list[str], batch_size: int, upstage_api_key: str) -> str:
    """ì—¬ëŸ¬ PDF íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ëª¨ë“  HTML ì½˜í…ì¸ ë¥¼ í†µí•©í•©ë‹ˆë‹¤."""
    print(f"\n--- ì´ {len(pdf_files)}ê°œ PDF íŒŒì¼ì˜ ì „ì²´ íŒŒì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
    all_html_content = ""
    for i, file_path in enumerate(pdf_files, 1):
        print(f"\n>>> íŒŒì¼ {i}/{len(pdf_files)} ì²˜ë¦¬ ì¤‘: {os.path.basename(file_path)}")
        html_content = parse_pdf_with_upstage(file_path, batch_size, upstage_api_key)
        if html_content:
            separator = f"\\n\\n<hr><h1>ë¬¸ì„œ: {os.path.basename(file_path)}</h1><hr>\\n\\n"
            all_html_content += separator + html_content
    print(f"\n--- ëª¨ë“  PDF ì²˜ë¦¬ ì™„ë£Œ. ì´ ì½˜í…ì¸  ê¸¸ì´: {len(all_html_content):,} ì ---")
    return all_html_content

# --- 3. LangChainì„ ì´ìš©í•œ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ---

def get_document_splits(html_content: str, chunk_size: int, chunk_overlap: int) -> list[Document]:
    """HTML ì½˜í…ì¸ ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜í•˜ê³  ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    print("\n--- LangChain ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘ ---")
    doc = Document(page_content=html_content, metadata={"source": "parsed_pdf_documents"})
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\\n\\n", "\\n", ". ", " ", ""]
    )
    
    document_list = text_splitter.split_documents([doc])
    print(f"  - ë¬¸ì„œë¥¼ ì´ {len(document_list)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    avg_chunk_len = len(html_content) // len(document_list) if document_list else 0
    print(f"  - í‰ê·  ì²­í¬ í¬ê¸°: {avg_chunk_len:,} ì")
    return document_list

def setup_vector_store(document_list: list[Document], index_name: str, pinecone_api_key: str) -> PineconeVectorStore:
    """ë¬¸ì„œ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  Pinecone ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„± ë° ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    print("\n--- ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ì‹œì‘ ---")
    
    embeddings = UpstageEmbeddings(model="solar-1-mini-embedding")
    print(f"  - ì„ë² ë”© ëª¨ë¸: solar-1-mini-embedding (ì°¨ì›: {embeddings.client.embedding_dimension})")

    pc = Pinecone(api_key=pinecone_api_key)
    
    # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆë‹¤ë©´ ì‚­ì œí•˜ì—¬ ìµœì‹  ìƒíƒœ ìœ ì§€
    if index_name in pc.list_indexes().names():
        print(f"  - ê¸°ì¡´ '{index_name}' ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
        pc.delete_index(index_name)
        time.sleep(5)  # ì‚­ì œ í›„ ì•ˆì •í™”ë¥¼ ìœ„í•´ ì ì‹œ ëŒ€ê¸°
    
    # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
    print(f"  - ìƒˆ '{index_name}' ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    pc.create_index(
        name=index_name,
        dimension=embeddings.client.embedding_dimension,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

    # ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ì—¬ ì¸ë±ìŠ¤ì— ì €ì¥
    print(f"  - {len(document_list)}ê°œì˜ ë¬¸ì„œë¥¼ ì¸ë±ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤...")
    vector_store = PineconeVectorStore.from_documents(
        documents=document_list,
        embedding=embeddings,
        index_name=index_name
    )
    print("  - âœ… ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ë° ë°ì´í„° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return vector_store

# --- 4. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---

def main():
    """ìŠ¤í¬ë¦½íŠ¸ì˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ìƒìˆ˜ ì •ì˜
    INDEX_NAME = "carbon-rag"
    CHUNK_SIZE = 1500
    CHUNK_OVERLAP = 200
    PDF_BATCH_SIZE = 10

    try:
        # 1. í™˜ê²½ ì„¤ì •
        upstage_api_key, pinecone_api_key = setup_environment()
        project_root = find_project_root()
        docs_folder = project_root / "docs"
        print(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
        
        # 2. ë¬¸ì„œ íŒŒì‹±
        pdf_files = get_pdf_files(docs_folder)
        html_content = parse_all_pdfs(pdf_files, PDF_BATCH_SIZE, upstage_api_key)
        if not html_content:
            raise ValueError("PDF íŒŒì¼ì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        # 3. ë¬¸ì„œ ë¶„í• 
        document_splits = get_document_splits(html_content, CHUNK_SIZE, CHUNK_OVERLAP)

        # 4. ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
        setup_vector_store(document_splits, INDEX_NAME, pinecone_api_key)
        
        print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ì´ì œ '{INDEX_NAME}' ì¸ë±ìŠ¤ë¥¼ RAG ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"\nâŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main() 